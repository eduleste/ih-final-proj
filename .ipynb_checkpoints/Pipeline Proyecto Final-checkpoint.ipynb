{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/eduardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eduardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1. Exploración y limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire(csv):\n",
    "    data = pd.read_csv(csv)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(df):\n",
    "    df_no_dupl = df.drop_duplicates()\n",
    "    return df_no_dupl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df_no_dupl, col, new_col):\n",
    "    df_no_dupl[new_col] = df_no_dupl[col].str.split().str.len()\n",
    "    return df_no_dupl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purge(df_no_dupl, new_col, n): # Eliminar filas con pocas palabras (no sirven)\n",
    "    df_purged = df_no_dupl[df_no_dupl[new_col] > n]\n",
    "    return df_purged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_column_for_mining(df_purged, col): # Funciona per devuelve pandas.core.series.Series\n",
    "    df_for_mining = df_purged[col]\n",
    "    return df_for_mining.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_spanish_text(df_for_mining, col, newcol):\n",
    "    df_for_mining[newcol] = df_for_mining[col].apply(detect)\n",
    "    df_spanish_only = df_for_mining[df_for_mining[newcol] != 'en']\n",
    "    return df_spanish_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2. Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(text): \n",
    "    return ''.join(ch for ch in text if ch.isalpha() or ch == ' ')\n",
    "\n",
    "def tokenizar(text): \n",
    "    token=ToktokTokenizer() #EN ppio deberia funcionar con castellano\n",
    "    word=token.tokenize(text)\n",
    "    return (word)\n",
    "\n",
    "def only_stem(word):  #Probar es-lemmatize y SpaCy\n",
    "    stemmer = SnowballStemmer('spanish', ignore_stopwords=True)\n",
    "    stem = [stemmer.stem(word) for word in word]\n",
    "    return \" \".join(stem).lower()\n",
    "\n",
    "def stopWordsRemove(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('spanish')])\n",
    "    return text\n",
    "\n",
    "def word_limitator(x):\n",
    "    limited_words = nltk.FreqDist([word for word in x.split()]).most_common(100)\n",
    "    return \" \".join([ word[0] for word in limited_words])\n",
    "\n",
    "def get_words(text): \n",
    "    limpio = clean_up(text)\n",
    "    lista = tokenizar(limpio)\n",
    "    lista2 = only_stem(lista)\n",
    "    res = stopWordsRemove(lista2)\n",
    "    resultado = word_limitator(res)\n",
    "    return resultado  #pandas.core.series.Series NECESARIO?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3. Clusterización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_to_numbers(resultado):\n",
    "    call_vectorizer=TfidfVectorizer()\n",
    "    train_vector=call_vectorizer.fit(resultado)\n",
    "    to_numbers=train_vector.transform(resultado)\n",
    "    transformed = pd.DataFrame(call_vectorizer.transform(resultado).todense(),columns=train_vector.get_feature_names())\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhoutte(to_numbers):\n",
    "    X = to_numbers\n",
    "    for n_cluster in range(50,51):\n",
    "        kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "        label = kmeans.labels_\n",
    "        sil_coeff = silhouette_score(X, label, metric='euclidean')\n",
    "    return print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elvow_curve(to_numbers, a , b):\n",
    "    X = to_numbers\n",
    "    nc=range(a, b)\n",
    "    kmeans = [KMeans(n_clusters=i) for i in nc]\n",
    "    score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n",
    "    score\n",
    "    plt.plot(nc,score)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Elbow Curve')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterization(transformed):\n",
    "    kmeans = KMeans(n_clusters = 47)\n",
    "    temp = kmeans.fit(transformed)\n",
    "    df_spanish_only['clusters'] = temp.labels_\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_PCA(transformed):\n",
    "    pca = PCA(n_components=20)\n",
    "    reduction = pca.fit_transform(transformed)  \n",
    "    return reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_col_to_reduction(reduction):\n",
    "    reduction['clusters'] = df_spanish_only['clusters']\n",
    "    return reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4. Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    X= reduction.drop(axis=1, labels=[\"clusters\"])\n",
    "    y= reduction.clusters\n",
    "    reduction, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    rfc=RandomForestClassifier(random_state=42)\n",
    "    param_grid = { 'n_estimators': [200, 500], 'max_features': ['auto', 'sqrt', 'log2'], \n",
    "              'max_depth' : [4,5,6,7,8], 'criterion' :['gini', 'entropy']}\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "    CV_rfc.fit(X_train, y_train)\n",
    "    CV_rfc.best_params_\n",
    "    rfc1=RandomForestClassifier(random_state=42, max_features='auto', \n",
    "                            n_estimators= 500, max_depth=8, criterion='entropy')\n",
    "    return print(\"Accuracy for Random Forest on CV data: \",accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
